{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from collections import defaultdict, Counter\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/cs224u/lib/python3.10/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "\t      negative: 3310\n",
      "\t       neutral: 1624\n",
      "\t      positive: 3610\n",
      "validation\n",
      "\t      negative: 428\n",
      "\t       neutral: 229\n",
      "\t      positive: 444\n",
      "train\n",
      "\t      negative: 14021\n",
      "\t       neutral: 45076\n",
      "\t      positive: 21391\n",
      "validation\n",
      "\t      negative: 1200\n",
      "\t       neutral: 1200\n",
      "\t      positive: 1200\n",
      "train\n",
      "\t      negative: 4579\n",
      "\t       neutral: 2448\n",
      "\t      positive: 6038\n",
      "validation\n",
      "\t      negative: 240\n",
      "\t       neutral: 240\n",
      "\t      positive: 240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/cs224u/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n"
     ]
    }
   ],
   "source": [
    "def print_label_dist(dataset, labelname='gold_label', splitnames=('train', 'validation')):\n",
    "    for splitname in splitnames:\n",
    "        print(splitname)\n",
    "        dist = sorted(Counter(dataset[splitname][labelname]).items())\n",
    "        for k, v in dist:\n",
    "            print(f\"\\t{k:>14s}: {v}\")\n",
    "\n",
    "dynasent_r1 = load_dataset(\"dynabench/dynasent\", 'dynabench.dynasent.r1.all')\n",
    "dynasent_r2 = load_dataset(\"dynabench/dynasent\", 'dynabench.dynasent.r2.all')\n",
    "sst = load_dataset(\"SetFit/sst5\")\n",
    "\n",
    "def convert_sst_label(s):\n",
    "    return s.split(\" \")[-1]\n",
    "\n",
    "for splitname in ('train', 'validation', 'test'):\n",
    "    dist = [convert_sst_label(s) for s in sst[splitname]['label_text']]\n",
    "    sst[splitname] = sst[splitname].add_column('gold_label', dist)\n",
    "    sst[splitname] = sst[splitname].add_column('sentence', sst[splitname]['text'])\n",
    "\n",
    "\n",
    "print_label_dist(sst)\n",
    "print_label_dist(dynasent_r1)\n",
    "print_label_dist(dynasent_r2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_shallow_neural_classifier import TorchShallowNeuralClassifier\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch.nn as nn\n",
    "   \n",
    "\n",
    "\n",
    "class BertClassifierModule(nn.Module):\n",
    "    def __init__(self, \n",
    "            n_classes, \n",
    "            hidden_activation, \n",
    "            weights_name=\"prajjwal1/bert-mini\",\n",
    "            max_model_length=512):\n",
    "        \"\"\"This module loads a Transformer based on  `weights_name`,\n",
    "        puts it in train mode, add a dense layer with activation\n",
    "        function give by `hidden_activation`, and puts a classifier\n",
    "        layer on top of that as the final output. The output of\n",
    "        the dense layer should have the same dimensionality as the\n",
    "        model input.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_classes : int\n",
    "            Number of classes for the output layer\n",
    "        hidden_activation : torch activation function\n",
    "            e.g., nn.Tanh()\n",
    "        weights_name : str\n",
    "            Name of pretrained model to load from Hugging Face\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.weights_name = weights_name\n",
    "        self.bert = AutoModel.from_pretrained(self.weights_name)\n",
    "        self.bert.train()\n",
    "        self.max_model_length = max_model_length\n",
    "        # for name, param in self.bert.named_parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.hidden_dim = self.bert.embeddings.word_embeddings.embedding_dim\n",
    "        \n",
    "        \n",
    "        self.classifier_layer = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim*self.max_model_length, self.hidden_dim),\n",
    "            self.hidden_activation,\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            self.hidden_activation,\n",
    "            nn.Linear(self.hidden_dim, self.n_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, indices, mask):\n",
    "        \"\"\"Process `indices` with `mask` by feeding these arguments\n",
    "        to `self.bert` and then feeding the initial hidden state\n",
    "        in `last_hidden_state` to `self.classifier_layer`\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        indices : tensor.LongTensor of shape (n_batch, k)\n",
    "            Indices into the `self.bert` embedding layer. `n_batch` is\n",
    "            the number of examples and `k` is the sequence length for\n",
    "            this batch\n",
    "        mask : tensor.LongTensor of shape (n_batch, d)\n",
    "            Binary vector indicating which values should be masked.\n",
    "            `n_batch` is the number of examples and `k` is the\n",
    "            sequence length for this batch\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tensor.FloatTensor\n",
    "            Predicted values, shape `(n_batch, self.n_classes)`\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # rep = self.bert(input_ids=indices, attention_mask=mask)\n",
    "        # return self.classifier_layer(rep.last_hidden_state[:,0,:])\n",
    "\n",
    "        rep = self.bert(input_ids=indices, attention_mask=mask)\n",
    "        return self.classifier_layer(torch.flatten(rep.last_hidden_state, start_dim=1))\n",
    "\n",
    "\n",
    "class BertClassifier(TorchShallowNeuralClassifier):\n",
    "    def __init__(self, weights_name, *args, **kwargs):\n",
    "        self.weights_name = weights_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.weights_name)\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.params += ['weights_name']\n",
    "\n",
    "    def build_graph(self):\n",
    "        return BertClassifierModule(\n",
    "            self.n_classes_, self.hidden_activation, self.weights_name, max_model_length=self.tokenizer.model_max_length)\n",
    "\n",
    "    def build_dataset(self, X, y=None):\n",
    "        \n",
    "        data = self.tokenizer(X, max_length=512, \n",
    "                    truncation=True, \n",
    "                    padding='max_length', \n",
    "                    add_special_tokens=True, \n",
    "                    return_attention_mask = True,\n",
    "                    return_tensors=\"pt\")\n",
    "        if y is None:\n",
    "            dataset = torch.utils.data.TensorDataset(\n",
    "                data['input_ids'], data['attention_mask'])\n",
    "        else:\n",
    "            self.classes_ = sorted(set(y))\n",
    "            self.n_classes_ = len(self.classes_)\n",
    "            class2index = dict(zip(self.classes_, range(self.n_classes_)))\n",
    "            y = [class2index[label] for label in y]\n",
    "            y = torch.tensor(y)\n",
    "            dataset = torch.utils.data.TensorDataset(\n",
    "                data['input_ids'], data['attention_mask'], y)\n",
    "        return dataset\n",
    "\n",
    "bert_finetune = BertClassifier(\n",
    "    weights_name=\"FacebookAI/roberta-base\",\n",
    "    max_iter=25,\n",
    "    hidden_activation=nn.ReLU(),\n",
    "    eta=0.00005,          # Low learning rate for effective fine-tuning.\n",
    "    batch_size=64,         # Small batches to avoid memory overload.\n",
    "    gradient_accumulation_steps=1,  # Increase the effective batch size to 32.\n",
    "    early_stopping=True,  # Early-stopping\n",
    "    n_iter_no_change=5)   # params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Finished epoch 1 of 25; error is 864.6148504018784Bad pipe message: %s [b'\\x8f\\xe8a>\\xe5\\x88%m\\x92sCFh\\x8b\\xd8\\xdd,\\xc7 T0-L\\xef/\\xb9P\\x9c\\x86,\\xd9<vjy\\xc9\\x82\\xe3\\xeb~\\x1f\\x7f\\x13\\x08\\xe1\\x85\\xfd\\xad\\x82\\x91f\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127', b'.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00']\n",
      "Bad pipe message: %s [b'\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01']\n",
      "Bad pipe message: %s [b\"\\x11\\x02\\x01\\xdb\\xdb\\xac\\x0c*:\\x08\\x9d\\x02\\xbfol\\x91,'\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\", b'', b'']\n",
      "Bad pipe message: %s [b'', b'\\x03\\x03']\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b\"\\x91\\xe2\\x8d\\xc4\\xb8\\x19(\\n\\x95*7\\xef\\xd3\\x83}\\x16\\n\\x1c\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\"]\n",
      "Bad pipe message: %s [b'', b'\\x02']\n",
      "Bad pipe message: %s [b'\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00\\xc0\\x00<\\x00\\xba\\x005\\x00\\x84\\x00/\\x00\\x96\\x00A\\x00\\x05\\x00\\n\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x03\\x03\\x02\\x03\\x03\\x01\\x02\\x01\\x03\\x02\\x02\\x02\\x04\\x02\\x05\\x02\\x06']\n",
      "Bad pipe message: %s [b'\\x05\\x02\\x06']\n",
      "Bad pipe message: %s [b'\\xcd\\xf6\\xb9|\\x12+\\x01\\x04^y\\xb1\\x16\\xd4z\\r\\xb6\\xbcv\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00', b'\\xff\\x02\\x01']\n",
      "Bad pipe message: %s [b\"\\xbb\\xfb\\xe3\\x9e\\x1ew&\\xc4\\x0cr9K4\\xefM\\xa3\\xe0'\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\"]\n",
      "Bad pipe message: %s [b'}\\xd3\\xb5\\xdd`\\xb1\\x8b\\x92\\n|\\xd0\\x19<_\\xa1\\xc5w\\x94\\x00\\x00>\\xc0']\n",
      "Bad pipe message: %s [b'\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t']\n",
      "Bad pipe message: %s [b'tn\\x12\\x88\\x12\\xff\\xa3e{\\xdd\\x0b\\xe9+Z\\xccrB\\x10\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x00', b'6\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x00']\n",
      "Bad pipe message: %s [b'\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03']\n",
      "Bad pipe message: %s [b'\\xf7\\xdf\\xa8A\\xb8\\x97\\x8c?JmT,\\t&\\xefH\\xa79\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00']\n",
      "Bad pipe message: %s [b'\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15']\n",
      "Bad pipe message: %s [b\"G\\xc539j}\\xe0|\\xaa\\xbd\\xaf-s\\x86\\xb1\\rg\\xba\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\", b'\\xff\\x02\\x01']\n",
      "Bad pipe message: %s [b\"\\x16\\xca(}\\xf3\\xd5\\x96\\x9a\\x8bpH\\x02C\\xef\\xec\\x98?M\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x00\\xa6\\x00l\\x004\\x00\\x9b\\x00F\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00\", b'\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00']\n",
      "Bad pipe message: %s [b'\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06']\n",
      "Stopping after epoch 9. Validation score did not improve by tol=1e-05 for more than 5 epochs. Final error is 159.7395149790682"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.797     0.746     0.771      1868\n",
      "     neutral      0.698     0.767     0.731      1669\n",
      "    positive      0.793     0.773     0.783      1884\n",
      "\n",
      "    accuracy                          0.762      5421\n",
      "   macro avg      0.763     0.762     0.762      5421\n",
      "weighted avg      0.765     0.762     0.763      5421\n",
      "\n",
      "CPU times: user 5h 4min 49s, sys: 44.4 s, total: 5h 5min 33s\n",
      "Wall time: 5h 5min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# FINAL MODEL\n",
    "# roberta base cased with flatten tensor instead of first token embedding + one more linear layer epochs to 25\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "_ = bert_finetune.fit(\n",
    "    sst['train']['sentence'] + dynasent_r1['train']['sentence'] + dynasent_r2['train']['sentence'],\n",
    "    sst['train']['gold_label'] + dynasent_r1['train']['gold_label'] + dynasent_r2['train']['gold_label'])\n",
    "\n",
    "preds = bert_finetune.predict(sst['validation']['sentence'] + dynasent_r1['validation']['sentence'] + dynasent_r2['validation']['sentence'])\n",
    "print(classification_report(sst['validation']['gold_label'] + dynasent_r1['validation']['gold_label'] + dynasent_r2['validation']['gold_label'], preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "bakeoff_df = pd.read_csv(\n",
    "    os.path.join(\"data\", \"sentiment\", \"cs224u-sentiment-test-unlabeled.csv\"))\n",
    "bakeoff_df['prediction'] = bert_finetune.predict(bakeoff_df[\"sentence\"].to_list())\n",
    "bakeoff_df.to_csv(\"data/sentiment/cs224u-sentiment-bakeoff-entry.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 10 of 10; error is 2.388294771371875"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.776     0.743     0.759       428\n",
      "     neutral      0.384     0.310     0.343       229\n",
      "    positive      0.743     0.847     0.792       444\n",
      "\n",
      "    accuracy                          0.695      1101\n",
      "   macro avg      0.634     0.633     0.631      1101\n",
      "weighted avg      0.681     0.695     0.686      1101\n",
      "\n",
      "CPU times: user 14min 42s, sys: 2.91 s, total: 14min 45s\n",
      "Wall time: 14min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# distill bert base cased with flatten tensor instead of first token embedding + one more linear layer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "_ = bert_finetune.fit(\n",
    "    sst['train']['sentence'],\n",
    "    sst['train']['gold_label'])\n",
    "\n",
    "preds = bert_finetune.predict(sst['validation']['sentence'])\n",
    "print(classification_report(sst['validation']['gold_label'], preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df44f8cb11ee416a9c857f1551c61c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Finished epoch 10 of 10; error is 9.714834146201613"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.782     0.811     0.796       428\n",
      "     neutral      0.405     0.328     0.362       229\n",
      "    positive      0.807     0.858     0.832       444\n",
      "\n",
      "    accuracy                          0.729      1101\n",
      "   macro avg      0.665     0.665     0.663      1101\n",
      "weighted avg      0.714     0.729     0.720      1101\n",
      "\n",
      "CPU times: user 28min 28s, sys: 6.14 s, total: 28min 34s\n",
      "Wall time: 28min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# roberta base with flatten tensor instead of first token embedding + one more linear layer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "_ = bert_finetune.fit(\n",
    "    sst['train']['sentence'],\n",
    "    sst['train']['gold_label'])\n",
    "\n",
    "preds = bert_finetune.predict(sst['validation']['sentence'])\n",
    "print(classification_report(sst['validation']['gold_label'], preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 10. Validation score did not improve by tol=1e-05 for more than 5 epochs. Final error is 5.969286805018783"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.693     0.706     0.699       428\n",
      "     neutral      0.335     0.384     0.358       229\n",
      "    positive      0.749     0.678     0.712       444\n",
      "\n",
      "    accuracy                          0.628      1101\n",
      "   macro avg      0.592     0.589     0.589      1101\n",
      "weighted avg      0.641     0.628     0.633      1101\n",
      "\n",
      "CPU times: user 1min 56s, sys: 166 ms, total: 1min 56s\n",
      "Wall time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# original bert mini with flatten tensor instead of first token embedding + one more linear layer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "_ = bert_finetune.fit(\n",
    "    sst['train']['sentence'],\n",
    "    sst['train']['gold_label'])\n",
    "\n",
    "preds = bert_finetune.predict(sst['validation']['sentence'])\n",
    "print(classification_report(sst['validation']['gold_label'], preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/cs224u/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Finished epoch 10 of 10; error is 8.079548183828592"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.641     0.752     0.692       428\n",
      "     neutral      0.319     0.227     0.265       229\n",
      "    positive      0.722     0.709     0.716       444\n",
      "\n",
      "    accuracy                          0.626      1101\n",
      "   macro avg      0.561     0.563     0.558      1101\n",
      "weighted avg      0.607     0.626     0.613      1101\n",
      "\n",
      "CPU times: user 1min 58s, sys: 1.44 s, total: 1min 59s\n",
      "Wall time: 1min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# original bert mini with flatten tensor instead of first token embedding\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "_ = bert_finetune.fit(\n",
    "    sst['train']['sentence'],\n",
    "    sst['train']['gold_label'])\n",
    "\n",
    "preds = bert_finetune.predict(sst['validation']['sentence'])\n",
    "print(classification_report(sst['validation']['gold_label'], preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 10 of 10; error is 106.32385301589966"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.551     0.752     0.636       428\n",
      "     neutral      0.500     0.083     0.142       229\n",
      "    positive      0.649     0.700     0.674       444\n",
      "\n",
      "    accuracy                          0.592      1101\n",
      "   macro avg      0.567     0.512     0.484      1101\n",
      "weighted avg      0.580     0.592     0.549      1101\n",
      "\n",
      "CPU times: user 45.3 s, sys: 208 ms, total: 45.6 s\n",
      "Wall time: 43.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# original bert mini with frozen backbone, lr*10\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "_ = bert_finetune.fit(\n",
    "    sst['train']['sentence'],\n",
    "    sst['train']['gold_label'])\n",
    "\n",
    "preds = bert_finetune.predict(sst['validation']['sentence'])\n",
    "print(classification_report(sst['validation']['gold_label'], preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/cs224u/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Finished epoch 10 of 10; error is 111.5887838602066"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.549     0.689     0.611       428\n",
      "     neutral      0.667     0.009     0.017       229\n",
      "    positive      0.597     0.755     0.667       444\n",
      "\n",
      "    accuracy                          0.574      1101\n",
      "   macro avg      0.604     0.484     0.432      1101\n",
      "weighted avg      0.593     0.574     0.510      1101\n",
      "\n",
      "CPU times: user 47.4 s, sys: 1.22 s, total: 48.6 s\n",
      "Wall time: 46.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# original bert mini with frozen backbone\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "_ = bert_finetune.fit(\n",
    "    sst['train']['sentence'],\n",
    "    sst['train']['gold_label'])\n",
    "\n",
    "preds = bert_finetune.predict(sst['validation']['sentence'])\n",
    "print(classification_report(sst['validation']['gold_label'], preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/cs224u/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Finished epoch 10 of 10; error is 28.556204199790955"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.717     0.682     0.699       428\n",
      "     neutral      0.313     0.245     0.275       229\n",
      "    positive      0.703     0.815     0.755       444\n",
      "\n",
      "    accuracy                          0.645      1101\n",
      "   macro avg      0.578     0.581     0.576      1101\n",
      "weighted avg      0.627     0.645     0.633      1101\n",
      "\n",
      "CPU times: user 1min 55s, sys: 1.33 s, total: 1min 56s\n",
      "Wall time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# bert mini with 1 more linear layer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "_ = bert_finetune.fit(\n",
    "    sst['train']['sentence'],\n",
    "    sst['train']['gold_label'])\n",
    "\n",
    "preds = bert_finetune.predict(sst['validation']['sentence'])\n",
    "print(classification_report(sst['validation']['gold_label'], preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 10 of 10; error is 4.913262668880634"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.796     0.675     0.731       428\n",
      "     neutral      0.380     0.380     0.380       229\n",
      "    positive      0.750     0.860     0.802       444\n",
      "\n",
      "    accuracy                          0.688      1101\n",
      "   macro avg      0.642     0.639     0.637      1101\n",
      "weighted avg      0.691     0.688     0.686      1101\n",
      "\n",
      "CPU times: user 13min 56s, sys: 3.51 s, total: 13min 59s\n",
      "Wall time: 13min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# distill bert base uncased\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "_ = bert_finetune.fit(\n",
    "    sst['train']['sentence'],\n",
    "    sst['train']['gold_label'])\n",
    "\n",
    "preds = bert_finetune.predict(sst['validation']['sentence'])\n",
    "print(classification_report(sst['validation']['gold_label'], preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/cs224u/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Finished epoch 10 of 10; error is 27.84134368598461"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.697     0.710     0.704       428\n",
      "     neutral      0.352     0.297     0.322       229\n",
      "    positive      0.710     0.755     0.731       444\n",
      "\n",
      "    accuracy                          0.642      1101\n",
      "   macro avg      0.586     0.587     0.586      1101\n",
      "weighted avg      0.631     0.642     0.636      1101\n",
      "\n",
      "CPU times: user 1min 55s, sys: 1.38 s, total: 1min 56s\n",
      "Wall time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#bert mini \n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "_ = bert_finetune.fit(\n",
    "    sst['train']['sentence'],\n",
    "    sst['train']['gold_label'])\n",
    "\n",
    "preds = bert_finetune.predict(sst['validation']['sentence'])\n",
    "print(classification_report(sst['validation']['gold_label'], preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 10 of 10; error is 64.46418231725693"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.787     0.578     0.666      1200\n",
      "     neutral      0.637     0.862     0.733      1200\n",
      "    positive      0.743     0.679     0.710      1200\n",
      "\n",
      "    accuracy                          0.706      3600\n",
      "   macro avg      0.722     0.706     0.703      3600\n",
      "weighted avg      0.722     0.706     0.703      3600\n",
      "\n",
      "CPU times: user 24min 41s, sys: 1.79 s, total: 24min 43s\n",
      "Wall time: 16min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# bert mini\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "_ = bert_finetune.fit(\n",
    "    dynasent_r1['train']['sentence'],\n",
    "    dynasent_r1['train']['gold_label'])\n",
    "\n",
    "preds = bert_finetune.predict(dynasent_r1['validation']['sentence'])\n",
    "print(classification_report(dynasent_r1['validation']['gold_label'], preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "bakeoff_df = pd.read_csv(\n",
    "    os.path.join(\"data\", \"sentiment\", \"cs224u-sentiment-test-unlabeled.csv\"))\n",
    "bakeoff_df['prediction'] = bert_finetune.predict(bakeoff_df[\"sentence\"].to_list())\n",
    "bakeoff_df.to_csv(\"data/sentiment/cs224u-sentiment-bakeoff-entry.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>This year we were at a restaurant that clearly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A long way.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A friend and I went on a Thursday evening  aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>You'll love to say I used to be married to tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I feel like any place I move will be a downgra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   example_id                                           sentence\n",
       "0           0  This year we were at a restaurant that clearly...\n",
       "1           1                                        A long way.\n",
       "2           2  A friend and I went on a Thursday evening  aro...\n",
       "3           3  You'll love to say I used to be married to tha...\n",
       "4           4  I feel like any place I move will be a downgra..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bakeoff_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bakeoff_df['prediction'] = bert_finetune.predict(bakeoff_df[\"sentence\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bakeoff_df.to_csv(\"data/sentiment/cs224u-sentiment-bakeoff-entry.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224u",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
