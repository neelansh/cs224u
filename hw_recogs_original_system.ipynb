{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92b51963-32f6-4dff-b009-26b68ae99006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import os\n",
    "import pandas as pd\n",
    "from compgen import recogs_exact_match\n",
    "\n",
    "SRC_DIRNAME = os.path.join(\"data\", \"recogs\")\n",
    "\n",
    "def load_split(filename):\n",
    "    return pd.read_csv(\n",
    "        filename,\n",
    "        delimiter=\"\\t\",\n",
    "        names=['input', 'output', 'category'])\n",
    "\n",
    "dataset = {}\n",
    "\n",
    "for splitname in (\"train\", \"dev\", \"gen\"):\n",
    "    dataset[splitname] = load_split(f\"{SRC_DIRNAME}/{splitname}.tsv\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7ee1488-878f-4d06-ac8f-78d1d3b1ae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import WhitespaceSplit\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "\n",
    "def get_tokenizer(vocab_filename):\n",
    "    with open(vocab_filename) as f:\n",
    "        vocab = f.read().splitlines()\n",
    "    vocab_size = len(vocab)\n",
    "    vocab = dict(zip(vocab, list(range(vocab_size))))\n",
    "    tok = Tokenizer(WordLevel(vocab, unk_token='[UNK]'))\n",
    "    # This definitely needs to be done here and in the construction of\n",
    "    # `PreTrainedTokenizerFast`. Don't be tempted to \"clean this up\"!\n",
    "    tok.add_special_tokens([\"[BOS]\", \"[UNK]\", \"[PAD]\", \"[EOS]\"])\n",
    "    tok.pre_tokenizer = WhitespaceSplit()\n",
    "    tok.post_processor = TemplateProcessing(\n",
    "        single=f\"[BOS]:0 $A:0 [EOS]:0\",\n",
    "        special_tokens=[\n",
    "            (\"[BOS]\", tok.token_to_id(\"[BOS]\")),\n",
    "            (\"[EOS]\", tok.token_to_id(\"[EOS]\"))])\n",
    "    return PreTrainedTokenizerFast(\n",
    "        tokenizer_object=tok,\n",
    "        bos_token=\"[BOS]\",\n",
    "        unk_token=\"[UNK]\",\n",
    "        pad_token=\"[PAD]\",\n",
    "        eos_token=\"[EOS]\",\n",
    "        # This vital; otherwise any periods will have their leading\n",
    "        # spaces removed, which is wrong for COGS/ReCOGS.\n",
    "        clean_up_tokenization_spaces=False)\n",
    "\n",
    "# enc_tokenizer = get_tokenizer(os.path.join(SRC_DIRNAME, \"src_vocab.txt\"))\n",
    "# dec_tokenizer = get_tokenizer(os.path.join(SRC_DIRNAME, \"tgt_vocab.txt\"))\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "class RecogsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, enc_tokenizer, dec_tokenizer, X, y=None):\n",
    "        self.X = [enc_tokenizer.encode(s) for s in X]\n",
    "        self.y = y\n",
    "        if y is not None:\n",
    "            self.y = [dec_tokenizer.encode(s) for s in y]\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        \"\"\"Unfortunately, we can't pass the tokenizer in as an argument\n",
    "        to this method, since it is a static method, so we need to do\n",
    "        the work of creating the necessary attention masks.\"\"\"\n",
    "        def get_pad_and_mask(vals):\n",
    "            lens = [len(i) for i in vals]\n",
    "            maxlen = max(lens)\n",
    "            pad = []\n",
    "            mask = []\n",
    "            for ex, length in zip(vals, lens):\n",
    "                diff = maxlen - length\n",
    "                pad.append(ex + ([0] * diff))\n",
    "                mask.append(([1] * length) + ([0] * diff))\n",
    "            return torch.tensor(pad), torch.tensor(mask)\n",
    "        batch_elements = list(zip(*batch))\n",
    "        X = batch_elements[0]\n",
    "        X_pad, X_mask = get_pad_and_mask(X)\n",
    "        if len(batch_elements) == 1:\n",
    "            # return X_pad, X_mask\n",
    "            return {\"input_ids\": X_pad, \"attention_mask\": X_mask}\n",
    "        else:\n",
    "            y = batch_elements[1]\n",
    "            y_pad, y_mask = get_pad_and_mask(y)\n",
    "            # Repeat `y_pad` because our optimizer expects to find\n",
    "            # labels in final position. These will not be used because\n",
    "            # Hugging Face will calculate the loss for us.\n",
    "            return {\"input_ids\": X_pad, \"attention_mask\": X_mask, \"labels\": y_pad}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return (self.X[idx],)\n",
    "        else:\n",
    "            return (self.X[idx], self.y[idx])\n",
    "\n",
    "\n",
    "from torch_model_base import TorchModelBase\n",
    "import torch.nn as nn\n",
    "from transformers import EncoderDecoderModel\n",
    "\n",
    "class RecogsLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reduction = \"mean\"\n",
    "\n",
    "    def forward(self, outputs, labels):\n",
    "        \"\"\"`labels` is ignored, as it was already used to assign a\n",
    "        value of `outputs.loss`, and that value is all we need.\"\"\"\n",
    "        return outputs.loss\n",
    "    \n",
    "class RecogsModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encdec = EncoderDecoderModel.from_pretrained(\n",
    "            f\"ReCOGS/ReCOGS-model\")\n",
    "\n",
    "    def forward(self, X_pad, X_mask, y_pad, y_mask, labels=None):\n",
    "        outputs = self.encdec(\n",
    "            input_ids=X_pad, \n",
    "            attention_mask=X_mask,\n",
    "            decoder_attention_mask=y_mask,\n",
    "            labels=y_pad)\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "class RecogsModel(TorchModelBase):\n",
    "    def __init__(self, *args,\n",
    "            initialize=True,\n",
    "            enc_vocab_filename=f\"{SRC_DIRNAME}/src_vocab.txt\",\n",
    "            dec_vocab_filename=f\"{SRC_DIRNAME}/tgt_vocab.txt\",\n",
    "            **kwargs):\n",
    "        self.enc_vocab_filename = enc_vocab_filename\n",
    "        self.dec_vocab_filename = dec_vocab_filename\n",
    "        self.enc_tokenizer = get_tokenizer(self.enc_vocab_filename)\n",
    "        self.dec_tokenizer = get_tokenizer(self.dec_vocab_filename)\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss = RecogsLoss()\n",
    "        if initialize:\n",
    "            self.initialize()\n",
    "\n",
    "    def build_graph(self):\n",
    "        return RecogsModule()\n",
    "\n",
    "    def build_dataset(self, X, y=None):\n",
    "        return RecogsDataset(\n",
    "            self.enc_tokenizer, self.dec_tokenizer, X, y=y)\n",
    "\n",
    "    def predict(self, X, device=None):\n",
    "        device = self.device if device is None else torch.device(device)\n",
    "        dataset = self.build_dataset(X)\n",
    "        dataloader = self._build_dataloader(dataset, shuffle=False)\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                X_pad, X_mask = [x.to(device) for x in batch]\n",
    "                outputs = self.model.encdec.generate(\n",
    "                    X_pad,\n",
    "                    attention_mask=X_mask,\n",
    "                    max_new_tokens=512,\n",
    "                    eos_token_id=self.model.encdec.config.eos_token_id)\n",
    "                results = self.dec_tokenizer.batch_decode(\n",
    "                    outputs, \n",
    "                    skip_special_tokens=True,\n",
    "                    clean_up_tokenization_spaces=False)\n",
    "                preds += results\n",
    "        return preds\n",
    "\n",
    "    def score(self, X, y, device=None):\n",
    "        # An overall accuracy score:\n",
    "        preds = self.predict(X, device=device)\n",
    "        vals = [int(recogs_exact_match(gold, pred)) for gold, pred in zip(y, preds)]\n",
    "        return sum(vals) / len(vals)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "448910a0-814a-4a16-ae82-e192333c5cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/cs224u/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/anaconda/envs/cs224u/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:640: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Stopping after epoch 20. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 15.865672990679741"
     ]
    }
   ],
   "source": [
    "# further model training\n",
    "\n",
    "recogs_ff = RecogsModel(\n",
    "    batch_size=512,\n",
    "    gradient_accumulation_steps=1,\n",
    "    max_iter=100, \n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=10,\n",
    "    optimizer_class=torch.optim.Adam,\n",
    "    eta=0.00001)\n",
    "\n",
    "_ = recogs_ff.fit(dataset['train'].input, dataset['train'].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "984e4f72-b14a-4edd-b095-115f9b2ca714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9663333333333334"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test(gen_df, model):\n",
    "    \n",
    "    cat_df = gen_df.copy()\n",
    "    cat_df[\"prediction\"] = model.predict(cat_df.input)\n",
    "    cat_df[\"correct\"] = cat_df.apply(lambda x: recogs_exact_match(x.output, x.prediction), axis=1)\n",
    "    return cat_df\n",
    "\n",
    "\n",
    "result_df = test(dataset['dev'], recogs_ff)\n",
    "result_df['correct'].sum() / result_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d583587f-c0af-441d-ac76-553f3191f6e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5645238095238095"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = test(dataset['dev'], recogs_ff)\n",
    "result_df['correct'].sum() / result_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a226ca1-9370-4ead-99f7-9f296157c523",
   "metadata": {},
   "outputs": [],
   "source": [
    "bakeoff_df = pd.read_csv(\n",
    "    os.path.join(SRC_DIRNAME, \"cs224u-recogs-test-unlabeled.tsv\"), \n",
    "    sep=\"\\t\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18aba84d-fa70-49d0-959f-0522a485c608",
   "metadata": {},
   "outputs": [],
   "source": [
    "bakeoff_df[\"prediction\"] = recogs_ff.predict(bakeoff_df.input)\n",
    "\n",
    "bakeoff_df.to_csv(\"cs224u-recogs-bakeoff-entry[fine_tuned_recogs_model].tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f17db12f-9414-4143-8cea-e1d28571f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T5 model\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "class T5RecogsModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encdec = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "\n",
    "    def forward(self, X_pad, X_mask, y_pad, y_mask, labels=None):\n",
    "        outputs = self.encdec(\n",
    "            input_ids=X_pad, \n",
    "            attention_mask=X_mask,\n",
    "            decoder_attention_mask=y_mask,\n",
    "            labels=y_pad)\n",
    "        return outputs\n",
    "\n",
    "class T5RecogsModel(RecogsModel):\n",
    "    def __init__(self, *args, initialize=True, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.enc_tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "        self.dec_tokenizer = self.enc_tokenizer\n",
    "\n",
    "    def build_graph(self):\n",
    "        return T5RecogsModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb8e2a79-e560-4dfd-998f-a537cd481389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c036af0dedf4d74ad747637edd30c01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450ae92dce2549cc9df0d9a282ddb7f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c3ce0bc7a345a5b4b08984cab75772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aaa4628a7bd455f84de6bf4db4ec150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d895adc4af7348e2bb34ae73462fbe8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54cecdf6c3924068b7aa5ff3f2079c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Liam hoffte, dass eine Box von einer Frau in der Hand gelegt wird.',\n",
       " 'Der Donkey lended den Cookie an eine Mutter .']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5mod = T5RecogsModel()\n",
    "t5_exs = dataset['dev'].input[: 2]\n",
    "\n",
    "t5_exs\n",
    "\n",
    "t5mod.predict(t5_exs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b0004c4-5ccc-4ad2-938f-5311bb0a77dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5model = T5RecogsModel(batch_size=64,\n",
    "    gradient_accumulation_steps=2,\n",
    "    max_iter=100, \n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=10,\n",
    "    optimizer_class=torch.optim.Adam,\n",
    "    eta=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e5d2db5-6601-4155-9609-50cd366f6cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 18. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 81.4184251409024"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5RecogsModel(\n",
       "\tbatch_size=64,\n",
       "\tmax_iter=100,\n",
       "\teta=1e-05,\n",
       "\toptimizer_class=<class 'torch.optim.adam.Adam'>,\n",
       "\tl2_strength=0,\n",
       "\tgradient_accumulation_steps=2,\n",
       "\tmax_grad_norm=None,\n",
       "\tvalidation_fraction=0.1,\n",
       "\tearly_stopping=True,\n",
       "\tn_iter_no_change=10,\n",
       "\twarm_start=False,\n",
       "\ttol=1e-05)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5model.fit(dataset['train'].input, dataset['train'].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "902bda9b-85ed-491f-8e8f-1276dcc1bc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev test acc:\n",
      "0.154\n",
      "gen test acc:\n",
      "0.0850952380952381\n"
     ]
    }
   ],
   "source": [
    "def test(gen_df, model):\n",
    "    \n",
    "    cat_df = gen_df.copy()\n",
    "    cat_df[\"prediction\"] = model.predict(cat_df.input)\n",
    "    cat_df[\"correct\"] = cat_df.apply(lambda x: recogs_exact_match(x.output, x.prediction), axis=1)\n",
    "    return cat_df\n",
    "\n",
    "\n",
    "result_df = test(dataset['dev'], t5model)\n",
    "print(\"dev test acc:\")\n",
    "print(result_df['correct'].sum() / result_df.shape[0])\n",
    "result_df = test(dataset['gen'], t5model)\n",
    "print(\"gen test acc:\")\n",
    "print(result_df['correct'].sum() / result_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf99b385-ef04-4d06-b721-19ebbe77a2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bakeoff_df = pd.read_csv(\n",
    "    os.path.join(SRC_DIRNAME, \"cs224u-recogs-test-unlabeled.tsv\"), \n",
    "    sep=\"\\t\", index_col=0)\n",
    "\n",
    "bakeoff_df[\"prediction\"] = t5model.predict(bakeoff_df.input)\n",
    "\n",
    "bakeoff_df.to_csv(\"cs224u-recogs-bakeoff-entry[fine_tuned_T5_model].tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b49990-3ffa-43d7-b19a-852b3df859a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base line default model result: \n",
    "# fine tune model: dev: 0.9663333333333334 gen: 0.5645238095238095\n",
    "# T5 model: dev: 0.154 gen: 0.0850952380952381\n",
    "# T5 with PEFT model\n",
    "\n",
    "# Fine tune model result: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "704637d3-025c-4895-8b3c-117173c4ca17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 589,824 || all params: 61,096,448 || trainable%: 0.9653981848502878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/cs224u/lib/python3.10/site-packages/peft/utils/other.py:145: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# huggingface hub model id\n",
    "model_id = \"google-t5/t5-small\"\n",
    "\n",
    "# load model from the hub\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=True, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n",
    "\n",
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    " r=16,\n",
    " lora_alpha=32,\n",
    " target_modules=[\"q\", \"v\"],\n",
    " lora_dropout=0.05,\n",
    " bias=\"none\",\n",
    " task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "# prepare int-8 model for training\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "# add LoRA adaptor\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "660e15ec-97e7-4ed7-a846-af09effbf196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9db2045c26a408184e9899f01e2a8e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/135546 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9cd0f9c351d490e87de5e1729b89958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SRC_DIRNAME = os.path.join(\"data\", \"recogs\")\n",
    "\n",
    "def load_split(filename):\n",
    "    return pd.read_csv(\n",
    "        filename,\n",
    "        delimiter=\"\\t\",\n",
    "        names=['input', 'output', 'category'])\n",
    "\n",
    "dataset = {}\n",
    "\n",
    "for splitname in (\"train\", \"dev\", \"gen\"):\n",
    "    dataset[splitname] = load_split(f\"{SRC_DIRNAME}/{splitname}.tsv\")\n",
    "    \n",
    "\n",
    "text_column = \"input\"\n",
    "label_column = \"output\"\n",
    "max_length = 512\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "dataset_train = Dataset.from_pandas(dataset[\"train\"], split=\"train\")\n",
    "dataset_eval = Dataset.from_pandas(dataset[\"dev\"], split=\"dev\")\n",
    "# dataset = concatenate_datasets([dataset_train, dataset_eval])\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[text_column]\n",
    "    targets = examples[label_column]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    labels = tokenizer(targets, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    labels = labels[\"input_ids\"]\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "dataset_train = dataset_train.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=dataset_train.column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "dataset_eval = dataset_eval.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=dataset_eval.column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62fcd77a-1917-4d9b-84f7-124eea9e673b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135546\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e109b741-fcbb-47c9-bbbe-c3db4f1a50ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bf2e5e1-7530-42be-8c29-e8211d2a6ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = RecogsDataset(\n",
    "#     enc_tokenizer,\n",
    "#     dec_tokenizer,\n",
    "#     dataset['train'].input,\n",
    "#     y=dataset['train'].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81f662b5-fe40-4437-911d-d75cbeedc0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_dataset = RecogsDataset(\n",
    "#     enc_tokenizer,\n",
    "#     dec_tokenizer,\n",
    "#     dataset['dev'].input,\n",
    "#     y=dataset['dev'].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c3d966a-8dc1-4fc3-8eac-b1c87d4cac91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex_dataloader = torch.utils.data.DataLoader(\n",
    "#     dev_dataset,\n",
    "#     batch_size=2,\n",
    "#     shuffle=True,\n",
    "#     pin_memory=True,\n",
    "#     collate_fn=dev_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02bc95e5-1c19-4549-9e64-dcfa48d47c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex_batch = iter(ex_dataloader)\n",
    "# next(ex_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32bac198-5c52-4c31-a9c4-64ec17b7e76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# # we want to ignore tokenizer pad token in the loss\n",
    "# label_pad_token_id = -100\n",
    "# # Data collator\n",
    "# data_collator = DataCollatorForSeq2Seq(\n",
    "#     enc_tokenizer,\n",
    "#     model=model,\n",
    "#     label_pad_token_id=label_pad_token_id,\n",
    "#     pad_to_multiple_of=8\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65df7620-9264-40b6-8edc-7b0d43279190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# model_id=\"t5-small\"\n",
    "\n",
    "# # Load tokenizer of FLAN-t5-XL\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a691dd7-51eb-480d-89b1-5ab8f10f1fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer(\"hi i am\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c11a9361-930f-4cc2-b6f5-3b3499abe90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "output_dir=\"lora-t5-small-2\"\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "\tper_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    learning_rate=1e-3, # higher learning rate\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    gradient_accumulation_steps=1,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "# trainer = Seq2SeqTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     data_collator=train_dataset.collate_fn,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=dev_dataset\n",
    "# )\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_eval\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2df37dc6-594b-4d7f-a734-1eda0abf0e61",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/cs224u/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10590' max='10590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10590/10590 3:48:23, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.329500</td>\n",
       "      <td>0.287091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.304100</td>\n",
       "      <td>0.286027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.296700</td>\n",
       "      <td>0.285313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.293900</td>\n",
       "      <td>0.285232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.291600</td>\n",
       "      <td>0.285085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/cs224u/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/anaconda/envs/cs224u/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/anaconda/envs/cs224u/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/anaconda/envs/cs224u/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10590, training_loss=0.3312184433761467, metrics={'train_runtime': 13705.5057, 'train_samples_per_second': 49.449, 'train_steps_per_second': 0.773, 'total_flos': 9.2953204752384e+16, 'train_loss': 0.3312184433761467, 'epoch': 5.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d5e35483-667a-475d-a69b-89ccc0978ab4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [03:03<00:00,  3.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.944\n"
     ]
    }
   ],
   "source": [
    "# model.eval()\n",
    "\n",
    "\n",
    "\n",
    "# result_df = test(dataset['dev'], recogs_ff)\n",
    "# result_df['correct'].sum() / result_df.shape[0]\n",
    "\n",
    "# def evaluate_peft_model(dev_set,max_target_length=500):\n",
    "#     # generate summary\n",
    "#     model.eval()  \n",
    "#     outputs = model.generate(input_ids=sample[\"input_ids\"].unsqueeze(0).cuda(), do_sample=True, top_p=0.9, max_new_tokens=max_target_length)\n",
    "#     print(outputs)\n",
    "#     prediction = dec_tokenizer.decode(outputs[0].detach().cpu().numpy(), skip_special_tokens=True)\n",
    "#     # decode eval sample\n",
    "#     # Replace -100 in the labels as we can't decode them.\n",
    "\n",
    "#     # Some simple post-processing\n",
    "#     return prediction\n",
    "\n",
    "\n",
    "# dev_dataloader = torch.utils.data.DataLoader(\n",
    "#                         dev_dataset,\n",
    "#                         batch_size=2,\n",
    "#                         shuffle=True,\n",
    "#                         pin_memory=True,\n",
    "#                         collate_fn=dev_dataset.collate_fn)\n",
    "    \n",
    "# for batch in dev_dataloader:\n",
    "#     preds = evaluate_peft_model(dev_dataset)\n",
    "import torch\n",
    "model.eval()\n",
    "def get_prediction(sample):\n",
    "    \n",
    "    # sample = tokenizer(sample, return_tensors='pt', max_length=512, truncation=True)\n",
    "    outputs = model.generate(input_ids=torch.tensor(sample[\"input_ids\"]).cuda(), \n",
    "                             attention_mask = torch.tensor(sample[\"attention_mask\"]).cuda(), \n",
    "                             do_sample=True, top_p=0.9, max_new_tokens=512)\n",
    "\n",
    "    prediction = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "predictions = []\n",
    "for i in tqdm(range(0, len(dataset_eval), 64)):\n",
    "    sample = dataset_eval[i:i+64]\n",
    "    pred = get_prediction(sample)\n",
    "    predictions+=pred\n",
    "    \n",
    "    \n",
    "cat_df = dataset[\"dev\"].copy() \n",
    "cat_df[\"prediction\"] = predictions\n",
    "cat_df[\"correct\"] = cat_df.apply(lambda x: recogs_exact_match(x.output, x.prediction), axis=1)\n",
    "print(cat_df['correct'].sum() / cat_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3efce733-4cec-4460-90bc-cb7df57d0d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24918cab7a02422da73184c38737ca73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/21000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = examples[text_column]\n",
    "    targets = examples[label_column]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    labels = tokenizer(targets, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    labels = labels[\"input_ids\"]\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "dataset_gen = Dataset.from_pandas(dataset[\"gen\"], split=\"gen\")\n",
    "dataset_gen = dataset_gen.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=dataset_gen.column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ef4e95d0-b6b3-44ac-80e4-37ea2df5725e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 329/329 [1:22:24<00:00, 15.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7460476190476191\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for i in tqdm(range(0, len(dataset_gen), 64)):\n",
    "    sample = dataset_gen[i:i+64]\n",
    "    pred = get_prediction(sample)\n",
    "    predictions+=pred\n",
    "    \n",
    "    \n",
    "cat_df = dataset[\"gen\"].copy() \n",
    "cat_df[\"prediction\"] = predictions\n",
    "cat_df[\"correct\"] = cat_df.apply(lambda x: recogs_exact_match(x.output, x.prediction), axis=1)\n",
    "print(cat_df['correct'].sum() / cat_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a112279c-73c6-43af-b4e4-d47cb1106ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8a65f791974e2798fc2fe8005ad219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/420 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]/anaconda/envs/cs224u/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "100%|██████████| 7/7 [00:59<00:00,  8.52s/it]\n"
     ]
    }
   ],
   "source": [
    "bakeoff_df = pd.read_csv(\n",
    "    os.path.join(SRC_DIRNAME, \"cs224u-recogs-test-unlabeled.tsv\"), \n",
    "    sep=\"\\t\", index_col=0)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[text_column]\n",
    "    # targets = examples[label_column]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    # labels = tokenizer(targets, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    # labels = labels[\"input_ids\"]\n",
    "    # labels[labels == tokenizer.pad_token_id] = -100\n",
    "    # model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "dataset_gen = Dataset.from_pandas(bakeoff_df, split=\"bakeoff\")\n",
    "dataset_gen = dataset_gen.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=dataset_gen.column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "predictions = []\n",
    "for i in tqdm(range(0, len(dataset_gen), 64)):\n",
    "    sample = dataset_gen[i:i+64]\n",
    "    pred = get_prediction(sample)\n",
    "    predictions+=pred\n",
    "\n",
    "bakeoff_df[\"prediction\"] = predictions\n",
    "\n",
    "bakeoff_df.to_csv(\"cs224u-recogs-bakeoff-entry[PEFT_T5_model].tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c001ec-55cb-4c69-91fc-883373850e05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ea12a87f-a837-44b1-a103-9ee1f70e87ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  1,  53, 592, 689, 124, 134, 586, 124, 361,  17,   2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  1, 103,   5,  52,   6,  67, 182,   5,  66,   6,  67, 381,   5,  46,\n",
      "           6,  67, 588,   5,  18,   6,  68, 177,   5,  18,   8,  52,   6,  68,\n",
      "         259,   5,  18,   8,  53,   6,  68, 579,   5,  53,   6,  68, 177,   5,\n",
      "          53,   8,  66,   6,  68, 664,   5,  53,   8,  46,   6,   2]])}\n",
      "tensor([[0, 1]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/cs224u/lib/python3.10/site-packages/transformers/generation/utils.py:1133: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dev_dataloader = torch.utils.data.DataLoader(\n",
    "                        dev_dataset,\n",
    "                        batch_size=1,\n",
    "                        shuffle=True,\n",
    "                        pin_memory=True,\n",
    "                        collate_fn=dev_dataset.collate_fn)\n",
    "\n",
    "for batch in dev_dataloader:\n",
    "    print(batch)\n",
    "    outputs = model.generate(input_ids=batch[\"input_ids\"].cuda())\n",
    "    print(outputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "14ff96d3-27ec-4299-8db7-edc86878c334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD] [BOS]'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_tokenizer.decode(outputs[0].detach().cpu().numpy(), skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee0f783-f22b-498b-a431-b9c5d227c83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_tokenizer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224u",
   "language": "python",
   "name": "cs224u"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
